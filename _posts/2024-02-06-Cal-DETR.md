---
title: "Cal-DETR: Calibrated Detection Transformer"
date: 2024-02-06 16:00:00 +09:00
categories: [Paper Reading]
tags:
   [
    Object Detection,
    DETR
   ]
use_math: true
--- 

# Cal-DETR: Calibrated Detection Transformer
[NeurIPS 2023](https://nips.cc/virtual/2023/poster/72856), 2024-02-06 기준 0회 인용

## Task
- Object Detection
- DETR
- Caalibration

## Contribution
- Safety-critical applications 에 Deep neural networks (DNN) 를 적용 및 활용하는데 있어 제한을 갖고 있다
> a deployed model could be exposed to constantly changing distributions which could be radically different from its training distribution.
- Proposing a mechanism for calibrated detection transformers (Cal-DETR)
- Uncertainty-guided logit modulation mechanism 제안
- Logit mixing approach 제안

## Proposed Method
<img src="/images/cal-detr/fig1.png" width="600px" alt="alt">

Uncalibrated - cat을 dog로 꽤 높은 confidence score를 기반으로 잘못 예측
Calibrated - cat으로 정확하게 예측

### Measuring Calibration

<img src="/images/cal-detr/eq1.png" width="550px" alt="alt">

Expected Calibration Error (ECE) - For Classification <br>
Datasets 을 $B$ 개수 만큼 나누어서 진행 - bin <br>
(Average accuracy - Average confidence) 에 각 bin 마다의 weights를 곱하고 sum <br>
-> miscalibration 를 meausre

<img src="/images/cal-detr/eq2.png" width="550px" alt="alt">

Detection Expected Calibration Error (D-ECE) - For Detection <br>
ECE 와 거의 동일한 형식이지만 Detection 에서는 Precision을 사용 <br>
mean Average Precision (mAP) 을 사용하지 않는 이유는 Recall 부분 때문

### Cal-DETR: Uncertainty-Guided Logit Modulation & Mixing

#### Uncertainty-Guided Logit Modulation
$O_D \in \mathbb{R}^{L \times M \times Q \times C}$ <br>
$L$ 개의 decoder layer 에 대한 결과 <br>
$M$ 은 mini-batch <br>
$C$ 는 Class number

$O_D^L \in \mathbb{R}^{M \times Q \times C}$ <br>
마지막 decoder layer 결과

$O_D \in \mathbb{R}^{L \times M \times Q \times C}$ 의 variance 를 계산 <br>
$u \in \mathbb{R}^{M \times Q \times C}$ -> uncertainty <br>
$1-\text{tanh}(u)$ -> certainty

$\tilde{O}_D^L = O_D^L \otimes (1-\text{tanh}(u))$

$L$ 개의 decoder layer 결과로부터 구해진 uncertainty 를 활용해서 certainty 를 구하고 마지막 decoder layer 결과에 곱한다

#### Logit Mixing for Confidence Calibration

<img src="/images/cal-detr/eq4.png" width="550px" alt="alt">

$Q_p = \{Q_1, Q_2, Q_3 \} \subset Q$ (total number of quries)

$\tilde{Q}$ 는 positive quries 들에 대한 평균 <br>
$\alpha$ 는 0.9 를 사용

이렇게 Logit Mixing 된 결과는 regularization loss 로 활용 <br>
label 은 $\alpha$ 로 smoothing (0.9)

$L_cls + \lambda L_R$ 으로 학습

>The proposed regularizer loss tends to capture the information from the vicinity of object instances in the logit space by mixing logits and constructing a non-zero entropy target distribution.

## Experimental Results

<img src="/images/cal-detr/tab1.png" width="650px" alt="alt">

Deformable-DETR 에 적용했을 때 D-ECE 가 감소

#### CorCOCO <br>
> The CorCOCO evaluation set is constructed by incorporating random severity and random corruption levels from [13]. <br>

[13] - Hendrycks, D. and T. Dietterich (2019). Benchmarking neural network robustness to common corruptions and perturbations. International Conference on Learning Representations (ICLR).

<img src="/images/cal-detr/corruption.png" width="350px" alt="alt">

[13] 논문에 명시되어 있는 Figure <br>
이와 같이 COCO 데이터셋을 random으로 nosie를 추가한 데이터셋 -> CorCOCO

<img src="/images/cal-detr/tab2.png" width="650px" alt="alt">

UP-DETR 에 적용했을 때 D-ECE 가 감소

<img src="/images/cal-detr/tab3.png" width="650px" alt="alt">

DINO 에도 똑같이 적용했을 때 D-ECE 감소